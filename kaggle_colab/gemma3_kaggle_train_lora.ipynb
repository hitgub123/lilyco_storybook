{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# 单元格 1: 安装所有必需的库\n",
    "# =======================================================================\n",
    "print(\"Installing libraries for QLoRA finetuning...\")\n",
    "!pip install -q -U transformers peft bitsandbytes datasets accelerate\n",
    "print(\"Installation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4bc5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# 单元格 2: 登录 Hugging Face Hub\n",
    "# =======================================================================\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "    print(\"Hugging Face token found. Logging in...\")\n",
    "    login(token=hf_token)\n",
    "    print(\"Login successful.\")\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"Could not log in to Hugging Face. Please ensure HUGGINGFACE_TOKEN is set correctly.\"\n",
    "    )\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# 单元格 3: 配置所有参数\n",
    "# =======================================================================\n",
    "class TrainingConfig:\n",
    "    # MODEL_ID = \"google/gemma-3-270m-it\"\n",
    "    # MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "    # MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "    # MODEL_ID = \"llama3.1:8b\"\n",
    "    MODEL_ID = \"phi4-mini:3.8b\"\n",
    "    DATA_FILE_PATH = (\n",
    "        \"/kaggle/input/test01/training_data_for_agent.jsonl\"  # <-- 请务必修改为您的路径\n",
    "    )\n",
    "    OUTPUT_DIR = f\"/kaggle/working/{MODEL_ID.replace('/','_')}_qlora_finetuned\"\n",
    "\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Model: {TrainingConfig.MODEL_ID}\")\n",
    "print(f\"  - Data file: {TrainingConfig.DATA_FILE_PATH}\")\n",
    "print(f\"  - Output directory: {TrainingConfig.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae387907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# 单元格 4: 主要的训练逻辑\n",
    "# =======================================================================\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# --- 数据加载和处理部分 ---\n",
    "print(f\"Loading dataset from {TrainingConfig.DATA_FILE_PATH}...\")\n",
    "dataset = load_dataset(\"json\", data_files=TrainingConfig.DATA_FILE_PATH, split=\"train\")\n",
    "print(f\"Dataset loaded with {len(dataset)} records.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TrainingConfig.MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = []\n",
    "    for prompt, tool_calls in zip(examples[\"prompt\"], examples[\"tool_calls\"]):\n",
    "        completion_obj = {\"tool_calls\": tool_calls}\n",
    "        completion_str = json.dumps(completion_obj, ensure_ascii=False)\n",
    "        text = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{completion_str}<end_of_turn>\"\n",
    "        texts.append(text)\n",
    "    tokenized_output = tokenizer(\n",
    "        texts, padding=\"longest\", truncation=True, max_length=512\n",
    "    )\n",
    "    tokenized_output[\"labels\"] = [x[:] for x in tokenized_output[\"input_ids\"]]\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=dataset.column_names\n",
    ")\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- QLoRA 配置 ---\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank, 可以设为 8, 16, 32等\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# --- 加载并准备模型 (采纳建议进行修改) ---\n",
    "print(\"Loading model with QLoRA configuration and best practices...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    TrainingConfig.MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=0,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"eager\",  # <-- 修改1: 使用 eager attention\n",
    "    use_cache=False,  # <-- 修改2: 明确禁用 use_cache\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Model prepared for QLoRA training.\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- 训练参数 (采纳建议进行修改) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TrainingConfig.OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_num_workers=0,\n",
    "    fp16=True,\n",
    "    # 禁用 wandb，防止卡死\n",
    "    report_to=\"none\",\n",
    "    # --- 修改3: 解决 use_reentrant 警告 ---\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a42400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 开始训练 ---\n",
    "print(\"Starting final training run...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 保存最终的 LoRA 适配器 ---\n",
    "print(f\"Saving final model adapters to {TrainingConfig.OUTPUT_DIR}...\")\n",
    "trainer.save_model(TrainingConfig.OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(TrainingConfig.OUTPUT_DIR)\n",
    "print(\"Script finished successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
