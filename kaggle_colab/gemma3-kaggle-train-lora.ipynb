{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12849339,"sourceType":"datasetVersion","datasetId":8121319}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bf6b828b","cell_type":"code","source":"# =======================================================================\n# 单元格 1: 安装所有必需的库\n# =======================================================================\nprint(\"Installing libraries for QLoRA finetuning...\")\n!pip install -q -U transformers peft bitsandbytes datasets accelerate ipdb wandb\nprint(\"Installation complete.\")\n# import ipdb  #for step run\n# ipdb.set_trace()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:41:48.852366Z","iopub.status.idle":"2025-08-24T05:41:48.852656Z","shell.execute_reply.started":"2025-08-24T05:41:48.852519Z","shell.execute_reply":"2025-08-24T05:41:48.852537Z"}},"outputs":[],"execution_count":null},{"id":"d3acbc71-25aa-4078-aa60-e90c5d9f18fb","cell_type":"code","source":"# 放在 notebook 比较靠前的位置\nimport os\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n    wandb.login(key=wandb_api_key)\n\n    # (可选) 将 W&B 设置为静默模式，减少不必要的输出\n    os.environ[\"WANDB_SILENT\"] = \"true\"\n\n    print(\"Successfully logged into W&B.\")\nexcept Exception as e:\n    print(f\"Could not log in to W&B. Please ensure the secret 'WANDB_API_KEY' is set correctly. Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:41:51.973236Z","iopub.execute_input":"2025-08-24T05:41:51.973721Z","iopub.status.idle":"2025-08-24T05:41:57.504099Z","shell.execute_reply.started":"2025-08-24T05:41:51.973677Z","shell.execute_reply":"2025-08-24T05:41:57.503391Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhotococoalj\u001b[0m (\u001b[33mhtcca\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged into W&B.\n","output_type":"stream"}],"execution_count":6},{"id":"7b4bc5cc","cell_type":"code","source":"# =======================================================================\n# 单元格 2: 登录 Hugging Face Hub\n# =======================================================================\nimport os\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n    print(\"Hugging Face token found. Logging in...\")\n    login(token=hf_token)\n    print(\"Login successful.\")\nexcept Exception as e:\n    print(\n        \"Could not log in to Hugging Face. Please ensure HUGGINGFACE_TOKEN is set correctly.\"\n    )\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:42:02.547998Z","iopub.execute_input":"2025-08-24T05:42:02.548658Z","iopub.status.idle":"2025-08-24T05:42:02.997828Z","shell.execute_reply.started":"2025-08-24T05:42:02.548632Z","shell.execute_reply":"2025-08-24T05:42:02.997061Z"}},"outputs":[{"name":"stdout","text":"Hugging Face token found. Logging in...\nLogin successful.\n","output_type":"stream"}],"execution_count":7},{"id":"a35b2c5a","cell_type":"code","source":"# =======================================================================\n# 单元格 3: 配置所有参数\n# =======================================================================\nclass TrainingConfig:\n    # MODEL_ID = \"google/gemma-3-270m-it\"\n    MODEL_ID = \"google/gemma-3-1b-it\"\n    # MODEL_ID = \"google/gemma-3-4b-it\"\n    # MODEL_ID = \"llama3.1:8b\"\n    # MODEL_ID = \"phi4-mini:3.8b\"\n    DATA_FILE_PATH = (\n        \"/kaggle/input/test01/training_data_for_agent.jsonl\"  # <-- 请务必修改为您的路径\n    )\n    OUTPUT_DIR = f\"/kaggle/working/{MODEL_ID.replace('/','_')}_qlora_finetuned\"\n\n\nprint(\"Training Configuration:\")\nprint(f\"  - Model: {TrainingConfig.MODEL_ID}\")\nprint(f\"  - Data file: {TrainingConfig.DATA_FILE_PATH}\")\nprint(f\"  - Output directory: {TrainingConfig.OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:42:07.406134Z","iopub.execute_input":"2025-08-24T05:42:07.406594Z","iopub.status.idle":"2025-08-24T05:42:07.411525Z","shell.execute_reply.started":"2025-08-24T05:42:07.406567Z","shell.execute_reply":"2025-08-24T05:42:07.410814Z"}},"outputs":[{"name":"stdout","text":"Training Configuration:\n  - Model: google/gemma-3-1b-it\n  - Data file: /kaggle/input/test01/training_data_for_agent.jsonl\n  - Output directory: /kaggle/working/google_gemma-3-1b-it_qlora_finetuned\n","output_type":"stream"}],"execution_count":8},{"id":"ae387907","cell_type":"code","source":"# =======================================================================\n# 单元格 4: 主要的训练逻辑\n# =======================================================================\nimport torch\nimport json\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\n# --- 数据加载和处理部分 ---\nprint(f\"Loading dataset from {TrainingConfig.DATA_FILE_PATH}...\")\ndataset = load_dataset(\"json\", data_files=TrainingConfig.DATA_FILE_PATH, split=\"train\")\nprint(f\"Dataset loaded with {len(dataset)} records.\")\n\ntokenizer = AutoTokenizer.from_pretrained(TrainingConfig.MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n\ndef tokenize_function(examples):\n    texts = []\n    for prompt, tool_calls in zip(examples[\"prompt\"], examples[\"tool_calls\"]):\n        completion_obj = {\"tool_calls\": tool_calls}\n        completion_str = json.dumps(completion_obj, ensure_ascii=False)\n        text = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{completion_str}<end_of_turn>\"\n        texts.append(text)\n    tokenized_output = tokenizer(\n        texts, padding=\"longest\", truncation=True, max_length=512\n    )\n    tokenized_output[\"labels\"] = [x[:] for x in tokenized_output[\"input_ids\"]]\n    return tokenized_output\n\n\nprint(\"Tokenizing dataset...\")\ntokenized_dataset = dataset.map(\n    tokenize_function, batched=True, remove_columns=dataset.column_names\n)\nprint(\"Tokenization complete.\")\n\n# --- QLoRA 配置 ---\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\nlora_config = LoraConfig(\n    r=16,  # LoRA rank, 可以设为 8, 16, 32等\n    lora_alpha=32,  # LoRA alpha\n    lora_dropout=0.05,\n    target_modules=[\n        \"q_proj\",\n        \"o_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    task_type=\"CAUSAL_LM\",\n)\n\n# --- 加载并准备模型 (采纳建议进行修改) ---\nprint(\"Loading model with QLoRA configuration and best practices...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    TrainingConfig.MODEL_ID,\n    quantization_config=quantization_config,\n    device_map=0,\n    torch_dtype=torch.float16,\n    attn_implementation=\"eager\",  # <-- 修改1: 使用 eager attention\n    # use_cache=False,  # <-- 修改2: 明确禁用 use_cache,没有这个参数\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nprint(\"Model prepared for QLoRA training.\")\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:42:09.878607Z","iopub.execute_input":"2025-08-24T05:42:09.878892Z","iopub.status.idle":"2025-08-24T05:42:38.681044Z","shell.execute_reply.started":"2025-08-24T05:42:09.878870Z","shell.execute_reply":"2025-08-24T05:42:38.680173Z"}},"outputs":[{"name":"stderr","text":"2025-08-24 05:42:15.813165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756014135.840458      90 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756014135.847253      90 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset from /kaggle/input/test01/training_data_for_agent.jsonl...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651fc220ad714751af27002349c19d8f"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded with 88 records.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5b5b6847584685b40feb618cab49c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6aac32b3f64f24aa61eb58673341fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6945826bdd0b4a98a282753a818aee04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dafb32e254e649e2a98fba1ca0e10389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98b982a9fb2449dc91f29cb27a4723ca"}},"metadata":{}},{"name":"stdout","text":"Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/88 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d31d363e6b854a1ba59c94c0c7617f6c"}},"metadata":{}},{"name":"stdout","text":"Tokenization complete.\nLoading model with QLoRA configuration and best practices...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e343318444a47ac85170dd21e8925ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd45a720b6e140818aceff0b84faaa0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0759b4c557c047f1a353d5e4f1b746e3"}},"metadata":{}},{"name":"stdout","text":"Model prepared for QLoRA training.\ntrainable params: 13,045,760 || all params: 1,012,931,712 || trainable%: 1.2879\n","output_type":"stream"}],"execution_count":9},{"id":"6b7c7b41-4e18-43da-89ba-783224857e94","cell_type":"code","source":"# --- 训练参数 (采纳建议进行修改) ---\ntraining_args = TrainingArguments(\n    output_dir=TrainingConfig.OUTPUT_DIR,\n    num_train_epochs=8,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    logging_strategy=\"steps\",\n    logging_steps=1,\n    save_strategy=\"epoch\",\n    dataloader_num_workers=0,\n    fp16=True,\n    # --- 修改3: 解决 use_reentrant 警告 ---\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    # 禁用 wandb，防止卡死\n    # report_to=\"none\",\n    report_to=[\"wandb\"],\n    # report_to=[\"tensorboard\",\"wandb\"], # <--- 修改这里\n    logging_dir=f\"{TrainingConfig.OUTPUT_DIR}/logs\", # 指定日志目录\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:46:07.036841Z","iopub.execute_input":"2025-08-24T05:46:07.037631Z","iopub.status.idle":"2025-08-24T05:46:07.087387Z","shell.execute_reply.started":"2025-08-24T05:46:07.037600Z","shell.execute_reply":"2025-08-24T05:46:07.086630Z"}},"outputs":[],"execution_count":14},{"id":"59a42400","cell_type":"code","source":"# --- 开始训练 ---\nprint(\"Starting final training run...\")\ntrainer.train()\nprint(\"Training finished.\")\n\n# --- 保存最终的 LoRA 适配器 ---\nprint(f\"Saving final model adapters to {TrainingConfig.OUTPUT_DIR}...\")\ntrainer.save_model(TrainingConfig.OUTPUT_DIR)\ntokenizer.save_pretrained(TrainingConfig.OUTPUT_DIR)\nprint(\"Script finished successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:46:10.476251Z","iopub.execute_input":"2025-08-24T05:46:10.476747Z","iopub.status.idle":"2025-08-24T05:48:04.649187Z","shell.execute_reply.started":"2025-08-24T05:46:10.476721Z","shell.execute_reply":"2025-08-24T05:48:04.648366Z"}},"outputs":[{"name":"stdout","text":"Starting final training run...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 01:49, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.525400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.578100</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.478600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.391300</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.348800</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.344600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.256700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.252800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.245200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.197500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.183500</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.172300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.172500</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.170900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.138500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.154300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.153800</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.116300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.117400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.138400</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.121000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.111800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.113000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.111700</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.138600</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.142800</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.115300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.115900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training finished.\nSaving final model adapters to /kaggle/working/google_gemma-3-1b-it_qlora_finetuned...\nScript finished successfully.\n","output_type":"stream"}],"execution_count":15},{"id":"6e4bbfae-8c74-432c-b6d2-8e1d0851e6da","cell_type":"code","source":"for i in trainer.state.log_history:\n    print(i)\n    \n# 从 log_history 中提取所有记录的 loss 值\nall_losses = [log['loss'] for log in trainer.state.log_history if 'loss' in log]\n\n# 计算真实的平均 loss\nif all_losses:\n    true_average_loss = sum(all_losses) / len(all_losses)\n    print(f\"\\\\nManually Calculated True Average Training Loss: {true_average_loss}\")\nelse:\n    print(\"\\\\nNo loss values were found in the log history.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:48:24.183280Z","iopub.execute_input":"2025-08-24T05:48:24.183557Z","iopub.status.idle":"2025-08-24T05:48:24.203936Z","shell.execute_reply.started":"2025-08-24T05:48:24.183537Z","shell.execute_reply":"2025-08-24T05:48:24.203356Z"}},"outputs":[{"name":"stdout","text":"{'loss': 0.5254, 'grad_norm': 111501.5078125, 'learning_rate': 0.0002, 'epoch': 0.36363636363636365, 'step': 1}\n{'loss': 2.5781, 'grad_norm': 9964107.0, 'learning_rate': 0.00019333333333333333, 'epoch': 0.7272727272727273, 'step': 2}\n{'loss': 0.4786, 'grad_norm': 223855.265625, 'learning_rate': 0.0001866666666666667, 'epoch': 1.0, 'step': 3}\n{'loss': 0.3913, 'grad_norm': 181938.9375, 'learning_rate': 0.00018, 'epoch': 1.3636363636363638, 'step': 4}\n{'loss': 0.3488, 'grad_norm': 196881.3125, 'learning_rate': 0.00017333333333333334, 'epoch': 1.7272727272727273, 'step': 5}\n{'loss': 0.3446, 'grad_norm': 132871.953125, 'learning_rate': 0.0001666666666666667, 'epoch': 2.0, 'step': 6}\n{'loss': 0.2567, 'grad_norm': 147286.703125, 'learning_rate': 0.00016, 'epoch': 2.3636363636363638, 'step': 7}\n{'loss': 0.2528, 'grad_norm': 104181.0703125, 'learning_rate': 0.00015333333333333334, 'epoch': 2.7272727272727275, 'step': 8}\n{'loss': 0.2452, 'grad_norm': 436097.65625, 'learning_rate': 0.00014666666666666666, 'epoch': 3.0, 'step': 9}\n{'loss': 0.1975, 'grad_norm': 56339.6953125, 'learning_rate': 0.00014, 'epoch': 3.3636363636363638, 'step': 10}\n{'loss': 0.1835, 'grad_norm': 282107.78125, 'learning_rate': 0.00013333333333333334, 'epoch': 3.7272727272727275, 'step': 11}\n{'loss': 0.1723, 'grad_norm': 86135.9296875, 'learning_rate': 0.00012666666666666666, 'epoch': 4.0, 'step': 12}\n{'loss': 0.1725, 'grad_norm': 81804.671875, 'learning_rate': 0.00012, 'epoch': 4.363636363636363, 'step': 13}\n{'loss': 0.1709, 'grad_norm': 533646.4375, 'learning_rate': 0.00011333333333333334, 'epoch': 4.7272727272727275, 'step': 14}\n{'loss': 0.1385, 'grad_norm': 138505.234375, 'learning_rate': 0.00010666666666666667, 'epoch': 5.0, 'step': 15}\n{'loss': 0.1634, 'grad_norm': 84658.8671875, 'learning_rate': 0.0001, 'epoch': 5.363636363636363, 'step': 16}\n{'loss': 0.1543, 'grad_norm': 88912.109375, 'learning_rate': 9.333333333333334e-05, 'epoch': 5.7272727272727275, 'step': 17}\n{'loss': 0.1538, 'grad_norm': 43608.2109375, 'learning_rate': 8.666666666666667e-05, 'epoch': 6.0, 'step': 18}\n{'loss': 0.1163, 'grad_norm': 156808.125, 'learning_rate': 8e-05, 'epoch': 6.363636363636363, 'step': 19}\n{'loss': 0.1174, 'grad_norm': 187874.125, 'learning_rate': 7.333333333333333e-05, 'epoch': 6.7272727272727275, 'step': 20}\n{'loss': 0.1384, 'grad_norm': 39172.11328125, 'learning_rate': 6.666666666666667e-05, 'epoch': 7.0, 'step': 21}\n{'loss': 0.121, 'grad_norm': 64740.79296875, 'learning_rate': 6e-05, 'epoch': 7.363636363636363, 'step': 22}\n{'loss': 0.1118, 'grad_norm': 109675.1640625, 'learning_rate': 5.333333333333333e-05, 'epoch': 7.7272727272727275, 'step': 23}\n{'loss': 0.113, 'grad_norm': 70384.875, 'learning_rate': 4.666666666666667e-05, 'epoch': 8.0, 'step': 24}\n{'loss': 0.1117, 'grad_norm': 55442.3671875, 'learning_rate': 4e-05, 'epoch': 8.363636363636363, 'step': 25}\n{'loss': 0.1386, 'grad_norm': 46559.67578125, 'learning_rate': 3.3333333333333335e-05, 'epoch': 8.727272727272727, 'step': 26}\n{'loss': 0.1428, 'grad_norm': 75614.3203125, 'learning_rate': 2.6666666666666667e-05, 'epoch': 9.0, 'step': 27}\n{'loss': 0.1293, 'grad_norm': 65066.53125, 'learning_rate': 2e-05, 'epoch': 9.363636363636363, 'step': 28}\n{'loss': 0.1153, 'grad_norm': 49072.671875, 'learning_rate': 1.3333333333333333e-05, 'epoch': 9.727272727272727, 'step': 29}\n{'loss': 0.1159, 'grad_norm': 38732.5546875, 'learning_rate': 6.666666666666667e-06, 'epoch': 10.0, 'step': 30}\n{'train_runtime': 112.8074, 'train_samples_per_second': 7.801, 'train_steps_per_second': 0.266, 'total_flos': 713216837836800.0, 'train_loss': 0.27998361761371293, 'epoch': 10.0, 'step': 30}\n\\nManually Calculated True Average Training Loss: 0.27998999999999996\n","output_type":"stream"}],"execution_count":16},{"id":"bc072423-e368-42ba-b7c9-317d5320e1d0","cell_type":"code","source":"#it does not work\n#%load_ext tensorboard\n#%tensorboard --logdir \"{TrainingConfig.OUTPUT_DIR}/logs\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c47c0fae-ca10-4556-8ad0-b9bf436923ba","cell_type":"code","source":"# =======================================================================\n# 合并基础模型与 LoRA 适配器\n# =======================================================================\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n \n# --- 配置路径 ---\n# 基础模型ID (必须和你训练时用的一致)\nbase_model_id = TrainingConfig.MODEL_ID\n\n# 你训练好的 LoRA 适配器路径 (即训练的输出目录)\nadapter_path = TrainingConfig.OUTPUT_DIR\n\n# 定义一个新目录，用于存放合并后的完整模型\nmerged_model_path = f\"/kaggle/working/{base_model_id.replace('/', '_')}_full_merged\"\n\nprint(f\"Base model: {base_model_id}\")\nprint(f\"Adapter path: {adapter_path}\")\nprint(f\"Merged model output path: {merged_model_path}\")\n\n\n# --- 加载并合并 ---\nprint(\"Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nprint(\"Loading LoRA adapter...\")\n# 加载适配器并将其应用到基础模型上\npeft_model = PeftModel.from_pretrained(base_model, adapter_path)\n\nprint(\"Merging adapter into the base model...\")\n# 执行合并，然后卸载适配器层，得到一个标准的 Transformer 模型\nmerged_model = peft_model.merge_and_unload()\nprint(\"Merge complete.\")\n\n# --- 保存完整的、可部署的模型 ---\nprint(f\"Saving merged model to {merged_model_path}...\")\nmerged_model.save_pretrained(merged_model_path)\n\n# 也要把分词器(tokenizer)保存到新目录中，它是模型的一部分\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\ntokenizer.save_pretrained(merged_model_path)\n\nprint(f\"Successfully saved the full merged model and tokenizer to {merged_model_path}\")\nprint(\"This is the directory you should package and use for Ollama.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:48:41.557163Z","iopub.execute_input":"2025-08-24T05:48:41.557833Z","iopub.status.idle":"2025-08-24T05:48:50.511760Z","shell.execute_reply.started":"2025-08-24T05:48:41.557800Z","shell.execute_reply":"2025-08-24T05:48:50.511074Z"}},"outputs":[{"name":"stdout","text":"Base model: google/gemma-3-1b-it\nAdapter path: /kaggle/working/google_gemma-3-1b-it_qlora_finetuned\nMerged model output path: /kaggle/working/google_gemma-3-1b-it_full_merged\nLoading base model...\nLoading LoRA adapter...\nMerging adapter into the base model...\nMerge complete.\nSaving merged model to /kaggle/working/google_gemma-3-1b-it_full_merged...\nSuccessfully saved the full merged model and tokenizer to /kaggle/working/google_gemma-3-1b-it_full_merged\nThis is the directory you should package and use for Ollama.\n","output_type":"stream"}],"execution_count":17},{"id":"8381228a-60b1-4c1e-ba2c-57c4282527d5","cell_type":"code","source":"# =======================================================================\n#打包并准备下载\n# =======================================================================\n\n# 你的模型输出目录（请确保这里的 MODEL_ID 和你训练时用的相匹配）\n# 这个变量在之前的单元格已经定义好了，这里只是为了清晰展示\n# model_id = \"google/gemma-3-4b-it\" \n# output_dir = f\"/kaggle/working/{model_id.replace('/', '_')}_qlora_finetuned\"\n\n# 要生成的压缩包文件名\narchive_name = f\"finetuned_{TrainingConfig.MODEL_ID.replace('/', '_')}.tar.gz\"\narchive_path = f\"/kaggle/working/{archive_name}\"\nsource_directory_to_archive = merged_model_path\n\nprint(f\"Output archive path: {archive_path}\")\nprint(f\"Archiving directory: {source_directory_to_archive}\")\n\n!rm -rf {TrainingConfig.OUTPUT_DIR}\n# 使用 tar 命令进行打包和压缩\n# -c: create an archive\n# -z: compress with gzip\n# -v: verbosely list files processed\n# -f: use archive file\n # 语法: tar -czvf [最终生成的压缩文件名] [要打包的源目录]\n# !tar -czvf {archive_path} {source_directory_to_archive}\n# 优化：使用 -C 参数，这可以防止在压缩包里创建多余的父文件夹层级，让解压后的目录更整洁。\n!tar -czvf {archive_path} -C {source_directory_to_archive} .\nprint(f\"Archive created successfully! You can now download '{archive_path}' .\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T05:49:00.650850Z","iopub.execute_input":"2025-08-24T05:49:00.651114Z","iopub.status.idle":"2025-08-24T05:51:30.030818Z","shell.execute_reply.started":"2025-08-24T05:49:00.651096Z","shell.execute_reply":"2025-08-24T05:51:30.030091Z"}},"outputs":[{"name":"stdout","text":"Output archive path: /kaggle/working/finetuned_google_gemma-3-1b-it.tar.gz\nArchiving directory: /kaggle/working/google_gemma-3-1b-it_full_merged\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"./\n./tokenizer.model\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"./config.json\n./chat_template.jinja\n./tokenizer_config.json\n./special_tokens_map.json\n./added_tokens.json\n./generation_config.json\n./model.safetensors\n./tokenizer.json\nArchive created successfully! You can now download '/kaggle/working/finetuned_google_gemma-3-1b-it.tar.gz' .\n","output_type":"stream"}],"execution_count":18},{"id":"b8173f3d-77dd-4797-baf2-398b2ba80ac2","cell_type":"code","source":"# --- 清理不再需要的文件夹以释放磁盘空间 ---\n# 1. 删除原始的 LoRA 适配器文件夹\nlora_adapter_dir = TrainingConfig.OUTPUT_DIR\nprint(f\"Deleting LoRA adapter directory: {lora_adapter_dir}...\")\n!rm -rf {lora_adapter_dir}\nprint(\"Done.\")\n\n\n# 2. 删除未打包的、合并后的模型文件夹\nmerged_dir = merged_model_path\nprint(f\"Deleting merged model directory: {merged_dir}...\")\n!rm -rf /kaggle/working/wandb\n!rm -rf {merged_dir}\nprint(\"Done.\")\nprint(\"\\nCleanup complete. Your /kaggle/working/ directory now only contains the final .tar.gz archive.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:00:36.170991Z","iopub.execute_input":"2025-08-24T06:00:36.171675Z","iopub.status.idle":"2025-08-24T06:00:36.889288Z","shell.execute_reply.started":"2025-08-24T06:00:36.171645Z","shell.execute_reply":"2025-08-24T06:00:36.888503Z"}},"outputs":[{"name":"stdout","text":"Deleting LoRA adapter directory: /kaggle/working/google_gemma-3-1b-it_qlora_finetuned...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Done.\nDeleting merged model directory: /kaggle/working/google_gemma-3-1b-it_full_merged...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Done.\n\nCleanup complete. Your /kaggle/working/ directory now only contains the final .tar.gz archive.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":21}]}